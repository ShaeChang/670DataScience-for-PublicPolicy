base_url <- "https://www2.census.gov/programs-surveys/demo/datasets/hhp/"
week_url <- "2021/wk40/HPS_Week40_PUF_CSV.zip"
pulse_url <- paste0(base_url, week_url)
# For Mac, *.nix systems:
download.file(
pulse_url,
destfile = "data/pulse40.zip"
)
base_url <- "https://www2.census.gov/programs-surveys/demo/datasets/hhp/"
week_url <- "2021/wk40/HPS_Week40_PUF_CSV.zip"
pulse_url <- paste0(base_url, week_url)
# For Mac, *.nix systems:
download.file(
pulse_url,
destfile = "data/pulse40.zip"
)
unzip(
zipfile = "data/pulze40.zip",
exdir = "data/")
base_url <- "https://www2.census.gov/programs-surveys/demo/datasets/hhp/"
week_url <- "2021/wk40/HPS_Week40_PUF_CSV.zip"
pulse_url <- paste0(base_url, week_url)
# For Mac, *.nix systems:
download.file(
pulse_url,
destfile = "data/pulse40.zip"
)
unzip(
zipfile = "data/pulse40.zip",
exdir = "data/")
pulse <- read.csv("data/pulse2021_puf_40.csv")
library(tidyverse)
base_url <- "https://www2.census.gov/programs-surveys/demo/datasets/hhp/"
week_url <- "2021/wk40/HPS_Week40_PUF_CSV.zip"
pulse_url <- paste0(base_url, week_url)
# For Mac, *.nix systems:
download.file(
pulse_url,
destfile = "data/pulse40.zip"
)
unzip(
zipfile = "data/pulse40.zip",
exdir = "data/")
#don't need any other folders before "data", because we are already in the week5_project folder.
pulse <- read.csv("data/pulse2021_puf_40.csv")
pulse
install.packages(janitor)
library(janitor)
install.packages("janitor")
?library()
library(janitor)
clean_names(pulse)
pulse <- read.csv("data/pulse2021_puf_40.csv") %>%
clean_names(pulse)
glimpse(pulse)
dc_centroids <- read_csv("data/geocorr2014_dc.csv")
install.packages(c("httr", "jsonlite"))
library(httr)
library(jsonlite)
library(tidyverse)
# make the URL
url <-
"https://api.census.gov/data/2014/pep/natstprc?get=STNAME,POP&DATE_=7&for=state:*"
# use the URL to make a request from the API
pop_json <- GET(url = url)
http_status(pop_json)
pop_data
# get the contents of the response as a text string
pop_json <- content(pop_json, as = "text")
pop_matrix <- fromJSON(pop_json)
# turn the body of the character matrix into a tibble
pop_data <- as_tibble(pop_matrix[2:nrow(pop_matrix), ],
.name_repair = "minimal")
# add variable names to the tibble
names(pop_data) <- pop_matrix[1, ]
pop_data
GET(url_link,
user_agent("Georgetown Univ. Student Data Collector (student@georgetown.edu)."))
ed_data <- GET(url = url,
user_agent("Georgetown Univ. Student Data Collector (xz551@georgetown.edu)."))
url <-
"https://educationdata.urban.org/api/v1/college-university/ipeds/enrollment-full-time-equivalent/2016/?level_of_study=2&fips=11"
## 这两个URL都可以用，有点奇怪哦"https://educationdata.urban.org/api/v1/college-university/ipeds/enrollment-full-time-equivalent/2016/2/?fips=11"
ed_data <- GET(url = url,
user_agent("Georgetown Univ. Student Data Collector (xz551@georgetown.edu)."))
http_status(ed_data)
library(sf)
install.packages(tigris)
library(tigris)
install.packages(tigris)
install.packages("tigris")
library(tigris)
?roads
DC_roads <- roads(state = "DC", county = "District of Columbia")
DC_roads <- roads(state = "DC", county = "District of Columbia")
DC_roadsmap <- ggplot(DC_roads) +
geom_sf() +
theme_void()
DC_roadsmap
?geom_sf
?geom_sf
mcbrocken <- read_csv(mcbroken.csv)
mcbroken_sf <- st_as_sf(mcbrocken, coords = c("lon", "lat"))
st_set_crs(value = 4326)
mcbrocken_map <- ggplot(mcbroken_sf) +
geom_sf(color = is_broken) +
theme_void()
mcbrocken_map
mcbrocken <- read_csv(mcbroken.csv)
mcbrocken <- read_csv(mcbroken)
mcbrocken <- read_csv("mcbroken.csv")
mcbroken_sf <- st_as_sf(mcbrocken, coords = c("lon", "lat"))
st_set_crs(value = 4326)
mcbrocken_map <- ggplot(mcbroken_sf) +
geom_sf(color = is_broken) +
theme_void()
mcbrocken_map
mcbroken_sf <- st_as_sf(mcbrocken, coords = c("lon", "lat")) %>%
st_set_crs(value = 4326)
mcbrocken_map <- ggplot(mcbroken_sf) +
geom_sf(color = is_broken) +
theme_void()
mcbrocken_map
mcbrocken_map <- ggplot(mcbroken_sf) +
geom_sf() +
theme_void()
mcbrocken_map
mcbrocken_map <- ggplot(mcbroken_sf, color = is_broken) +
geom_sf() +
theme_void()
mcbrocken_map
library(rsample)
library(parsnip)
library(recipes)
library(workflows)
library(tune)
library(yardstick)
library(rsample)
install.packages("rsample")
library(rsample)
install.packages("parsnip")
library(parsnip)
install.packages("recipes")
library(recipes)
install.packages("workflows")
install.packages("workflows")
library(workflows)
install.packages("tune")
library(tune)
install.packages("yardstick")
library(yardstick)
set.seed(20201004)
x <- runif(n = 1000, min = 0, max = 10)
data1 <- bind_cols(
x = x,
y = 10 * sin(x) + x + 20 + rnorm(n = length(x), mean = 0, sd = 2)
)
set.seed(20201007)
# create a split object
data1_split <- initial_split(data = data1, prop = 0.75)
# create the training and testing data
data1_train <- training(x = data1_split)
data1_test  <- testing(x = data1_split)
# visualize the data
data1_train %>%
ggplot(aes(x = x, y = y)) +
geom_point(alpha = 0.25) +
labs(title = "Example 1 Data") +
theme_minimal()
library(ggplot2)
data1_train %>%
ggplot(aes(x = x, y = y)) +
geom_point(alpha = 0.25) +
labs(title = "Example 1 Data") +
data1_train %>%
ggplot(aes(x = x, y = y)) +
geom_point(alpha = 0.25) +
labs(title = "Example 1 Data") +
theme_minimal()
library(ggplot2)
data1_train %>%
ggplot(aes(x = x, y = y)) +
geom_point(alpha = 0.25) +
labs(title = "Example 1 Data") +
theme_minimal()
# create a knn model specification
knn_mod <-
nearest_neighbor(neighbors = 5) %>%
set_engine(engine = "kknn") %>%
set_mode(mode = "regression")
# fit the knn model specification on the training data
knn_fit <- knn_mod %>%
fit(formula = y ~ x, data = data1_train)
install.packages("kknn")
library(kknn)
knn_mod <-
nearest_neighbor(neighbors = 5) %>%
set_engine(engine = "kknn") %>%
set_mode(mode = "regression")
# fit the knn model specification on the training data
knn_fit <- knn_mod %>%
fit(formula = y ~ x, data = data1_train)
